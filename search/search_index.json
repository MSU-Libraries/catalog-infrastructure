{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome","text":"<p>This is the technical user documentation for the infrastructure behind the Michigan State University Libraries' public catalog system. It describes how to set up and configure the infrastructure used for the containerized VuFind system.</p> <p>Feel free to reach out to our team with any questions or suggestions you have!</p>"},{"location":"#first-time-setup","title":"First Time Setup","text":"<ul> <li>AWS User Setup</li> <li>GitLab Setup</li> <li>Deploy User Setup</li> <li>Terraform Setup</li> <li>DNS Setup</li> </ul>"},{"location":"#how-to-guides","title":"How To Guides","text":"<ul> <li>Manually run terraform commands</li> <li>Increasing node root partition size</li> <li>Force recreation of a node</li> <li>Mounting the shared storage</li> </ul>"},{"location":"#developing","title":"Developing","text":"<ul> <li>Documentation</li> </ul>"},{"location":"documentation/","title":"Documentation","text":""},{"location":"documentation/#github-pages","title":"GitHub Pages","text":"<p>This site is hosted on GitHub Pages. It is build and deployed via GitHub Actions.</p>"},{"location":"documentation/#troubleshooting","title":"Troubleshooting","text":"<p>If there are issues with the page displaying there are two things you can try.</p> <ol> <li> <p>Change the branch in the Pages Settings    to something other than <code>gh-pages</code>, save it, then wait a few minutes, then    change it back to <code>gh-pages</code> and make sure the path is <code>/ (root)</code> and save    it again. It will take a few minutes for the GitHub Action to run to    redeploy the update before you can check again.</p> </li> <li> <p>If that does not work, verify the output of the GitHub Action job log to make    sure there were no errors building the documentation site. Find the most    recent pipeline    then click into it, and then into the <code>deploy</code> job. You can expand each    section to see messages. The one most likely to cause issues would be the step    called <code>Run mkdocs gh-deploy --force</code>. There could be errors listed like    syntax errors in the <code>.md</code> files, or possibly even just re-running    the job could resolve the issue (maybe in combination with repeating    step 1 afterwards).</p> </li> </ol>"},{"location":"documentation/#building-locally-for-testing","title":"Building locally for testing","text":"<p>It is possible to build the GitHub pages documentation site locally on your machine to test it before you commit and deploy it.</p> <pre><code>pip install mkdocs-material mkdocs-autorefs mkdocs-material-extensions mkdocstrings\ncd ${CATALOG_INFRA_REPO_DIR}\npython3 -m mkdocs serve\n# OR\npython3 -m mkdocs serve -a localhost:9000\n</code></pre> <p>You should now be able to open your browser to the link in the <code>serve</code> output to verify. Additionally, the output of the serve command would display any errors with building the site.</p> <p>As long as the <code>serve</code> command is running, it will auto-detect changes to the files so you can continue to modify them and see the updates in your browser.</p>"},{"location":"documentation/#running-checks-on-markdown-files","title":"Running checks on Markdown files","text":"<pre><code>cd ${CATALOG_REPO_DIR}\n# Lint checks\ndocker run --rm -it \\\n  -v $PWD:/code \\\n  registry.gitlab.com/pipeline-components/markdownlint-cli2:latest \\\n  markdownlint-cli2 \"**/**.md\"\n\n# Spell checks\ndocker run --rm -it \\\n  -v $PWD:/code \\\n  registry.gitlab.com/pipeline-components/markdown-spellcheck:latest \\\n  mdspell --report '**/*.md' --ignore-numbers --ignore-acronyms\n</code></pre>"},{"location":"first-time-setup/","title":"First time setup","text":""},{"location":"first-time-setup/#aws-user-setup","title":"AWS User Setup","text":"<p>An AWS user is required and will need to have multiple access roles granted to it in IAM to perform the tasks required in Terraform. Below outline the permissions required on the group the AWS user is attached to:  </p> <p>Policy: <code>AmazonEC2FullAccess</code> Custom Policy</p> <p>If you are using the above IAM policy as a template for yourself, be sure to modify any IDs or resource names as needed (some are specifically named <code>msul</code>).  </p> <p>Save the AWS Key and Secret for this user to be set in the GitLab CI/CD variables.</p>"},{"location":"first-time-setup/#gitlab-setup","title":"GitLab Setup","text":"<p>GitLab is not required for using this repository -- it simply helps call all of the steps in the correct order with the right parameters. If you want to use this repository without GitLab you can still references the <code>.gitlab-ci.yml</code> file to help understand what steps you would need to manually run in order to spin up your cluster(s). Like how to use the below variables that would be in the CI/CD settings.</p> <p>The following CI/CD variables must also be created: (note that GitLab supports defining different values for the same variable for each environment, so you can have a <code>DEPLOY_HOST_1,2,3</code> for devel and different values for prod (and different values of the <code>MAIN_TF_FILE</code> for shared).</p> <ul> <li><code>AWS_KEY</code>: The AWS Key for the user with the above user-policy</li> <li><code>AWS_SECRET</code>: The AWS Secret for the user with the above user-policy</li> <li><code>ROOT_PRIVATE_KEY</code>: This is the <code>base64</code> encoded private key, the public key is in the Terraform environment definititions</li> <li><code>DEPLOY_HOST_1</code>: The first node in the cluster (i.e. catprod-1.aws.lib.msu.edu)</li> <li><code>DEPLOY_HOST_2</code>: The second node in the cluster</li> <li><code>DEPLOY_HOST_3</code>: The third node in the cluster</li> <li><code>GITHUB_USER_TOKEN</code>: Token used to publish releases to GitHub repository</li> <li><code>VARIABLES_YAML_FILE</code>:  Containing the completed contents of the  <code>configure-playbook/variables.yml.example</code>   file leaving in <code>$</code> variable references like <code>REGISTRY_ACCESS_TOKEN</code>. Concentrate on completeing the   <code>create_users</code> section with all the users that should have access to the nodes and their <code>public_keys</code> they will use.</li> <li><code>MAIN_TF_FILE</code>: Containing the completed contents of <code>terraform/env/{devel,prod,shared}/main.tf.example</code> (remember, this can be a scoped variable)</li> <li><code>RW_CICD_TOKEN</code>: Read-Write access token to this repository used to create release tags</li> </ul>"},{"location":"first-time-setup/#msul-users","title":"MSUL Users","text":"<p>The user running the pipeline needs to have access to read from the following repositories:</p> <ul> <li>(optional) playbook-ubuntu-setup<ul> <li>This is an optional stage in the pipleine and not required.</li> <li>This requires the public key for the <code>ROOT_PRIVATE_KEY</code> to be manually added to the   <code>ubuntusetup</code> user's authorized keys file on the <code>ansible.lib.msu.edu</code> server for the CI to connect.</li> <li>The CI will connect with that key but then from the <code>ansible.lib.msu.edu</code> server it will connect   to the catalog nodes as the <code>ansible</code> user using the key created in the <code>user_data.sh</code> script in Terraform   (the public key is passed in via the Terraform environment definitions and the private key is stored only   on the <code>ansible.lib.msu.edu</code> server in the <code>ansible</code> user's <code>.ssh</code> directory.</li> </ul> </li> </ul>"},{"location":"first-time-setup/#deploy-user-setup","title":"Deploy User Setup","text":"<p>A deploy key has been created and it's public key is stored in the <code>configure-playbook/variables.yml</code> CI variable, <code>VARIABLE_YAML_FILE</code> with a corresponding private key in the CI/CD variables of the catalog project's repository. Should that key ever need to change, both locations will need to be updated in the <code>DEPLOY_PRIVATE_KEY</code> variable there.</p> <p>This key is used in this repository when the <code>configure-playbook</code> is run and is setting up users; it will setup the authorized key entry for that public key on the <code>deploy</code> user. Then the catalog repository uses that when it connects to the codes as the deploy user to deploy the Docker stacks.</p>"},{"location":"first-time-setup/#terraform-setup","title":"Terraform Setup","text":"<p>This repository contains 3 Terraform environments: shared, devel, and prod. The shared one represents shared resources accross all the clusters, such as the storage. Those resources must be created first before either devel or prod are created.</p> <p>The 3 environments we have created in this repository represent specific settings that made sense for us. To make your own cluster, you will want to copy the shared environment and the devel and/or prod directories and start modifying them. Here are the values you are most likely going to want to change in each:</p> <p>Shared</p> <ul> <li><code>terraform</code><ul> <li><code>backend \"s3\"</code>: This identifies where the Terraform state file is stored, you'll want to make this a bucket and key specific to you</li> </ul> </li> <li><code>module \"shared\"</code><ul> <li><code>alert_emails</code>: What distribution list will receive alert emails from AWS</li> <li><code>domain</code>: The domain to create the resources in</li> <li><code>zone_id</code>: The DNS zone ID in Route53 that the <code>domain</code> is associated with</li> </ul> </li> <li><code>module \"cluster\"</code><ul> <li><code>root_public_key</code>: Public SSH key for the root user on the nodes created earlier in the <code>ROOT_PRIVATE_KEY</code></li> <li><code>ansible_public_key</code>: Public SSH key for the ansible server that will run ubuntu-setup-playbook.   This is MSUL specific and should be left blank for other users.</li> <li><code>net_allow_inbound_ssh</code>: CIDR range that allows connections from port 22 (ssh)</li> <li><code>net_allow_inbound_ncpa</code>: CIDR range that allows connections from port 5693 (for Nagios NCPA monitoring)</li> <li><code>net_allow_inbound_web</code>: CIDR range that allows connections from port 443 and 80</li> <li><code>roundrobin_hostnames</code>: DNS names that you want created to resolve to one of the 3 nodes in the cluster</li> <li><code>nodes</code><ul> <li><code>server_name</code>: Name of the server in the cluster</li> <li><code>aws_ami</code>: The AWS AMI for the instance you want to create (this is like the VM image tag)</li> <li><code>aws_instance_type</code>: Type of EC2 instance to create</li> <li><code>aws_root_block_size</code>: Size of the root partition of the node</li> <li><code>cpu_balance_threshold</code>: What CPU credit balance threshold must be met for an alert to be sent</li> <li><code>ebs_balance_threshold</code>: What percentage threshold of the burst balance for an alert to be sent</li> </ul> </li> </ul> </li> </ul> <p>Also of note, we have two cases where we have <code>prevent_destroy</code> set to avoid accidently destroying critical resources (our EFS share and the EC2 instances), such as a bad commit in a CI deploy. But if you are testing and want to be able to remove those, you will need to look for those references and set it to <code>false</code>.</p>"},{"location":"first-time-setup/#dns-setup","title":"DNS Setup","text":"<p>Since this terraform playbook only creates DNS entries in the <code>.aws.lib.msu.edu</code> domain (see the <code>domain</code> variable in the terraform files), and we want our site to be accessible at <code>catalog.lib.msu.edu</code> (and not just <code>catalog.aws.lib.msu.edu</code>), we need to create CNAME entries in our local DNS server that point to the ones that AWS created.</p> <p>For example, we have the following CNAME records:</p> Name Type Target catalog Alias (CNAME) catprod.aws.lib.msu.edu. catalog-beta Alias (CNAME) catprod.aws.lib.msu.edu. catalog-preview Alias (CNAME) catprod.aws.lib.msu.edu. catalog-prod Alias (CNAME) catprod.aws.lib.msu.edu."},{"location":"force-recreation-of-a-node/","title":"Force recreation of a node","text":"<p>NOTE: This whole process takes about 10-15 minutes.</p> <ul> <li> <p>Leave the swarm <pre><code># Run this on the node\ndocker swarm leave\n\n# Run this on this on another node\ndocker node rm --force [name/ID]\n</code></pre></p> </li> <li> <p>Use terraform to replace the node <pre><code># Got to the directory of the environment with the node you want to replace\ncd terraform/env/prod\n\n# This example is replacing node \"c\"\nterraform apply -replace='module.catalog.module.nodes[\"c\"].aws_instance.node_instance'\n</code></pre></p> </li> <li> <p>Use the AWS console (or be patient) to confirm the new node is ready You can confirm the new node is ready in the EC2 console by viewing the system logs for the instance. This typically takes 5-10 minutes. Or of course, you could just wait that amount of time and skip to the next step hoping for the best!</p> </li> <li> <p>Re-run the last successful provision pipeline This step is required as it will run the playbooks required to configure the newly created node.</p> </li> </ul>"},{"location":"increasing-node-root-partition-size/","title":"Increasing node root partition size","text":"<p>Increasing the size of the root EBS block, used for the root file-system, can be done relatively quickly and without requiring a reboot.</p> <ol> <li>Increase the <code>aws_root_block_size</code> (GB) for each node in <code>terraform/env/prod/main.tf</code></li> <li>Commit and push to <code>main</code> branch. This will trigger <code>terraform</code> to perform the disk expansion via the CI pipeline.<ul> <li>You could also manually run <code>terraform apply</code> on <code>terraform/env/prod/</code>; just be sure to commit and push afterwards.</li> </ul> </li> <li>On each node:<ul> <li>Expand the partition table. This can be done via:   <pre><code>parted --list\n# When prompted to Fix/Ignore, choose fix\nWarning: Not all of the space available to /dev/nvme0n1 appears to be used, you\ncan fix the GPT to use all of the space (an extra 402653184 blocks) or continue\nwith the current setting?\nFix/Ignore? fix\n</code></pre></li> <li>Expand the root partition (typically partition 1; see output of previous command):   <pre><code>parted /dev/nvme0n1\n# Will take you to parted prompt\n(parted) resizepart\nPartition number? 1\nWarning: Partition /dev/nvme0n1p1 is being used. Are you sure you want to continue?\nYes/No? yes\nEnd?  [68.7GB]? 100%\n(parted) quit\n</code></pre></li> <li>Expand the ext4 file-system to use 100% of the partition (default action for <code>resize2fs</code>):   <pre><code>resize2fs /dev/nvme0n1p1\n</code></pre></li> </ul> </li> </ol> <p>At this point the file-system should be expanded. Verify via <code>df -h</code>.</p>"},{"location":"mannually-run-terraform-commands/","title":"Manually run terraform commands","text":"<pre><code># Change directory to the environment you want to apply changes to\ncd terraform/env/prod\n\n# Preview Changes\nterraform plan\n\n# Apply Changes\nterraform apply\n\n# Take down the server cluster and all resources associated with it\nterraform destroy\n\n# Re-initialize new cluster if a node is re-created without removing from swarm first\ndocker swarm init --force-new-cluster \n</code></pre>"},{"location":"mounting-the-shared-storage/","title":"Mounting the Shared Storage","text":"<p>To mount the shared storage (<code>/mnt/shared/local</code> on the nodes) on your local machine, use the credentials for the same user as used by Solr and the Traefik Dashboard). To mount, we will be making use of the <code>sshfs</code> tool.</p> <p>For more information on this share see the technical documentation.</p> <p>Permissions on Server The shared storage on the server will need to be writable by the connecting user. For example, if the storage us group writable by the <code>ubuntu</code> user, the user on the server will needt o be in the <code>ubuntu</code> group. This can be done by adding the group to the user in <code>configure-playbook/variables.yml</code> CI variable, <code>VARIABLE_YAML_FILE</code> and then triggering the CI process. <pre><code> - name: myuser\n   comment: 'Myu Ser'\n   groups: ubuntu\n   public_keys:\n</code></pre></p>"},{"location":"mounting-the-shared-storage/#macos","title":"MacOS","text":"<p>Mac users will require macFUSE and sshfs to be installed separately before continuing. The latest release is available on the osxfuse GitHub site.</p> <p>Locally on the Mac mounting the share, you'll have to make it think it also has permissions to write. For example, if the server has the <code>ubunbu</code> group as writable with gid of <code>1000</code>, you'll need to configure you Mac to also have a group with gid <code>1000</code> that your user</p> <p>To create the new group and add your user <code>myuser</code>: <pre><code># Create group ubuntu with gid 1000\nsudo dscl . -create /groups/ubuntu gid 1000\n# Add user myuser to ubuntu group\nsudo dscl . -append /Groups/ubuntu GroupMembership myuser\n</code></pre></p> <p>From there you should be able to create a directory to mount from and mount as your normal user (no <code>sudo</code>): <pre><code># Only need to create the directory the first time\nmkdir ~/my-sshfs\n\nsshfs -o allow_other,default_permissions myuser@catprod-1.aws.lib.msu.edu:/mnt/shared/local ~/my-sshfs\n</code></pre></p> <p>To unmount: <pre><code>umount ~/my-sshfs \n</code></pre></p>"},{"location":"mounting-the-shared-storage/#linux","title":"Linux","text":"<p>Before attempting to mount, you will need <code>sshfs</code>. To install: <pre><code>$ sudo apt install sshfs\n</code></pre></p> <p>Here is an example of mounting the share (to and example <code>/mnt/point/</code> directory) for a single time (not auto-remounted): <pre><code>sudo sshfs -o allow_other,default_permissions [netid]@catprod-1.aws.lib.msu.edu:/mnt/shared/local /mnt/point\n</code></pre></p> <p>Here is an example of an <code>/etc/fstab</code> entry for mounting it: <pre><code>[netid]@catprod-1.aws.lib.msu.edu:/mnt/shared/local /mnt/point fuse.sshfs noauto,x-systemd.automount,_netdev,reconnect,identityfile=/home/[netid]/.ssh/id_ed25519,allow_other,default_permissions 0 0\n</code></pre></p> <p>To mount: <pre><code>$ sudo mkdir -p /mnt/catalog\n$ sudo mount /mnt/catalog\n$ sudo umount /mnt/catalog\n</code></pre></p>"},{"location":"traefik-lets-encrypt-certificates-not-renewing-automatically/","title":"Traefik Let's Encrypt certificates not renewing automatically","text":"<p>Traefik runs as two separate services on the cluster, one as the keymaster, which requests the certificate, and another traefik service that simply serves certificates. Each Traefik container runs on only a single node, so only 1 node is assigned the responsibility of requesting certificates.</p> <p>There is a separate service called cert-sync that copies the certificates nightly from the keymaster to shared storage and from the shared storage to each of the other Traefik container's volumes.</p> <p>Notes on this approach:</p> <ul> <li>If a new or renewed certificate is requested, it may take a few days   for the other nodes to get it because the first night will be   when it syncs it to the shared storage and the second night the other   containers will get it.</li> <li>When new certificates are copied on to the non-keymaster containers,   they need to be restarted to recognize these new certificates. There is   currently not an automatic process for this because Let's Encypt requests   new certificates one month before they expire, so the logic being that we have   a month for those certificates to sync to the non-keymaster nodes and for those   containers to restart (due to re-deploy, Docker restart, or server reboot).</li> </ul> <p>If you need to manually sync the certicates faster you can:</p> <ul> <li>Run this command three times to re-deploy the sync service:</li> </ul> <pre><code>docker service rm cert-sync_cert-sync; sudo -Hu deploy docker stack deploy -c /home/deploy/core-stacks/docker-compose.cert-sync.yml cert-sync\n</code></pre> <ul> <li>Restart the Traefik containers on the non-keymaster nodes:</li> </ul> <pre><code># Run on nodes 2 and 3 in the cluster\ndocker stop $(docker ps -q -f name=traefik_traefik)\n</code></pre>"}]}