{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>This is the technical user documentation for the infrastructure behind the Michigan State University Libraries' public catalog system. It describes how to set up and configure the infrastructure used for the containerized VuFind system.</p> <p>Feel free to reach out to our team with any questions or suggestions you have!</p>"},{"location":"#first-time-setup","title":"First Time Setup","text":"<ul> <li>AWS User Setup</li> <li>GitLab Setup</li> <li>Deploy User Setup</li> <li>Terraform Setup</li> <li>DNS Setup</li> </ul>"},{"location":"#how-to-guides","title":"How To Guides","text":"<ul> <li>Manually run terraform commands</li> <li>Increasing node root partition size</li> <li>Force recreation of a node</li> <li>Mounting the shared storage</li> </ul>"},{"location":"#developing","title":"Developing","text":"<ul> <li>Documentation</li> </ul>"},{"location":"documentation/","title":"Documentation","text":""},{"location":"documentation/#github-pages","title":"GitHub Pages","text":"<p>This site is hosted on GitHub Pages. It is build and deployed via GitHub Actions.</p>"},{"location":"documentation/#troubleshooting","title":"Troubleshooting","text":"<p>If there are issues with the page displaying there are two things you can try.</p> <ol> <li> <p>Change the branch in the Pages Settings    to something other than <code>gh-pages</code>, save it, then wait a few minutes, then    change it back to <code>gh-pages</code> and make sure the path is <code>/ (root)</code> and save    it again. It will take a few minutes for the GitHub Action to run to    redeploy the update before you can check again.</p> </li> <li> <p>If that does not work, verify the output of the GitHub Action job log to make    sure there were no errors building the documentation site. Find the most    recent pipeline    then click into it, and then into the <code>deploy</code> job. You can expand each    section to see messages. The one most likely to cause issues would be the step    called <code>Run mkdocs gh-deploy --force</code>. There could be errors listed like    syntax errors in the <code>.md</code> files, or possibly even just re-running    the job could resolve the issue (maybe in combination with repeating    step 1 afterwards).</p> </li> </ol>"},{"location":"documentation/#building-locally-for-testing","title":"Building locally for testing","text":"<p>It is possible to build the GitHub pages documentation site locally on your machine to test it before you commit and deploy it.</p> <pre><code>pip install mkdocs-material mkdocs-autorefs mkdocs-material-extensions mkdocstrings\ncd ${CATALOG_INFRA_REPO_DIR}\npython3 -m mkdocs serve\n# OR\npython3 -m mkdocs serve -a localhost:9000\n</code></pre> <p>You should now be able to open your browser to the link in the <code>serve</code> output to verify. Additionally, the output of the serve command would display any errors with building the site.</p> <p>As long as the <code>serve</code> command is running, it will auto-detect changes to the files so you can continue to modify them and see the updates in your browser.</p>"},{"location":"documentation/#running-checks-on-markdown-files","title":"Running checks on Markdown files","text":"<pre><code>cd ${CATALOG_REPO_DIR}\n# Lint checks\ndocker run --rm -it \\\n  -v $PWD:/code \\\n  registry.gitlab.com/pipeline-components/markdownlint-cli2:latest \\\n  markdownlint-cli2 \"**/**.md\"\n\n# Spell checks\ndocker run --rm -it \\\n  -v $PWD:/code \\\n  registry.gitlab.com/pipeline-components/markdown-spellcheck:latest \\\n  mdspell --report '**/*.md' --ignore-numbers --ignore-acronyms\n</code></pre>"},{"location":"first-time-setup/","title":"First time setup","text":""},{"location":"first-time-setup/#aws-user-setup","title":"AWS User Setup","text":"<p>An AWS user is required and will need to have multiple access roles granted to it in IAM to perform the tasks required in Terraform. Below outline the permissions required on the group the AWS user is attached to:  </p> <p>Policy: <code>AmazonEC2FullAccess</code> Custom Policy</p> <p>If you are using the above IAM policy as a template for yourself, be sure to modify any IDs or resource names as needed (some are specifically named <code>msul</code>).  </p> <p>Save the AWS Key and Secret for this user to be set in the GitLab CI/CD variables.</p>"},{"location":"first-time-setup/#gitlab-setup","title":"GitLab Setup","text":"<p>GitLab is not required for using this repository -- it simply helps call all of the steps in the correct order with the right parameters. If you want to use this repository without GitLab you can still references the <code>.gitlab-ci.yml</code> file to help understand what steps you would need to manually run in order to spin up your cluster(s). Like how to use the below variables that would be in the CI/CD settings.</p> <p>The following CI/CD variables must also be created: (note that GitLab supports defining different values for the same variable for each environment, so you can have a <code>DEPLOY_HOST_1,2,3</code> for devel and different values for prod (and different values of the <code>MAIN_TF_FILE</code> for shared).</p> <ul> <li><code>AWS_KEY</code>: The AWS Key for the user with the above user-policy</li> <li><code>AWS_SECRET</code>: The AWS Secret for the user with the above user-policy</li> <li><code>ROOT_PRIVATE_KEY</code>: This is the <code>base64</code> encoded private key, the public key is in the Terraform environment definititions</li> <li><code>DEPLOY_HOST_1</code>: The first node in the cluster (i.e. catalog-1.aws.lib.msu.edu)</li> <li><code>DEPLOY_HOST_2</code>: The second node in the cluster</li> <li><code>DEPLOY_HOST_3</code>: The third node in the cluster</li> <li><code>GITHUB_USER_TOKEN</code>: Token used to publish releases to GitHub repository</li> <li><code>VARIABLES_YAML_FILE</code>:  Containing the completed contents of the  <code>configure-playbook/variables.yml.example</code>   file leaving in <code>$</code> variable references like <code>REGISTRY_ACCESS_TOKEN</code>. Concentrate on completeing the   <code>create_users</code> section with all the users that should have access to the nodes and their <code>public_keys</code> they will use.</li> <li><code>MAIN_TF_FILE</code>: Containing the completed contents of <code>terraform/env/{devel,prod,shared}/main.tf.example</code> (remember, this can be a scoped variable)</li> <li><code>RW_CICD_TOKEN</code>: Read-Write access token to this repository used to create release tags</li> </ul>"},{"location":"first-time-setup/#msul-users","title":"MSUL Users","text":"<p>The user running the pipeline needs to have access to read from the following repositories:</p> <ul> <li>(optional) playbook-ubuntu-setup<ul> <li>This is an optional stage in the pipleine and not required.</li> <li>This requires the public key for the <code>ROOT_PRIVATE_KEY</code> to be manually added to the   <code>ubuntusetup</code> user's authorized keys file on the <code>ansible.lib.msu.edu</code> server for the CI to connect.</li> <li>The CI will connect with that key but then from the <code>ansible.lib.msu.edu</code> server it will connect   to the catalog nodes as the <code>ansible</code> user using the key created in the <code>user_data.sh</code> script in Terraform   (the public key is passed in via the Terraform environment definitions and the private key is stored only   on the <code>ansible.lib.msu.edu</code> server in the <code>ansible</code> user's <code>.ssh</code> directory.</li> </ul> </li> </ul>"},{"location":"first-time-setup/#deploy-user-setup","title":"Deploy User Setup","text":"<p>A deploy key has been created and it's public key is stored in the <code>configure-playbook/variables.yml</code> CI variable, <code>VARIABLE_YAML_FILE</code> with a corresponding private key in the CI/CD variables of the catalog project's repository. Should that key ever need to change, both locations will need to be updated in the <code>DEPLOY_PRIVATE_KEY</code> variable there.</p> <p>This key is used in this repository when the <code>configure-playbook</code> is run and is setting up users; it will setup the authorized key entry for that public key on the <code>deploy</code> user. Then the catalog repository uses that when it connects to the codes as the deploy user to deploy the Docker stacks.</p>"},{"location":"first-time-setup/#terraform-setup","title":"Terraform Setup","text":"<p>This repository contains 3 Terraform environments: shared, devel, and prod. The shared one represents shared resources accross all the clusters, such as the storage. Those resources must be created first before either devel or prod are created.</p> <p>The 3 environments we have created in this repository represent specific settings that made sense for us. To make your own cluster, you will want to copy the shared environment and the devel and/or prod directories and start modifying them. Here are the values you are most likely going to want to change in each:</p> <p>Shared</p> <ul> <li><code>terraform</code><ul> <li><code>backend \"s3\"</code>: This identifies where the Terraform state file is stored, you'll want to make this a bucket and key specific to you</li> </ul> </li> <li><code>module \"shared\"</code><ul> <li><code>alert_emails</code>: What distribution list will receive alert emails from AWS</li> <li><code>domain</code>: The domain to create the resources in</li> <li><code>zone_id</code>: The DNS zone ID in Route53 that the <code>domain</code> is associated with</li> </ul> </li> <li><code>module \"cluster\"</code><ul> <li><code>root_public_key</code>: Public SSH key for the root user on the nodes created earlier in the <code>ROOT_PRIVATE_KEY</code></li> <li><code>ansible_public_key</code>: Public SSH key for the ansible server that will run ubuntu-setup-playbook.   This is MSUL specific and should be left blank for other users.</li> <li><code>net_allow_inbound_ssh</code>: CIDR range that allows connections from port 22 (ssh)</li> <li><code>net_allow_inbound_ncpa</code>: CIDR range that allows connections from port 5693 (for Nagios NCPA monitoring)</li> <li><code>net_allow_inbound_web</code>: CIDR range that allows connections from port 443 and 80</li> <li><code>roundrobin_hostnames</code>: DNS names that you want created to resolve to one of the 3 nodes in the cluster</li> <li><code>nodes</code><ul> <li><code>server_name</code>: Name of the server in the cluster</li> <li><code>aws_ami</code>: The AWS AMI for the instance you want to create (this is like the VM image tag)</li> <li><code>aws_instance_type</code>: Type of EC2 instance to create</li> <li><code>aws_root_block_size</code>: Size of the root partition of the node</li> <li><code>cpu_balance_threshold</code>: What CPU credit balance threshold must be met for an alert to be sent</li> <li><code>ebs_balance_threshold</code>: What percentage threshold of the burst balance for an alert to be sent</li> </ul> </li> </ul> </li> </ul> <p>Also of note, we have two cases where we have <code>prevent_destroy</code> set to avoid accidently destroying critical resources (our EFS share and the EC2 instances), such as a bad commit in a CI deploy. But if you are testing and want to be able to remove those, you will need to look for those references and set it to <code>false</code>.</p>"},{"location":"first-time-setup/#dns-setup","title":"DNS Setup","text":"<p>Since this terraform playbook only creates DNS entries in the <code>.aws.lib.msu.edu</code> domain (see the <code>domain</code> variable in the terraform files), and we want our site to be accessible at <code>catalog.lib.msu.edu</code> (and not just <code>catalog.aws.lib.msu.edu</code>), we need to create CNAME entries in our local DNS server that point to the ones that AWS created.</p> <p>For example, we have the following CNAME records:</p> Name Type Target catalog Alias (CNAME) catalog.aws.lib.msu.edu. catalog-beta Alias (CNAME) catalog.aws.lib.msu.edu. catalog-preview Alias (CNAME) catalog.aws.lib.msu.edu. catalog-prod Alias (CNAME) catalog.aws.lib.msu.edu."},{"location":"force-recreation-of-a-node/","title":"Force recreation of a node","text":"<p>NOTE: This whole process takes about 10-15 minutes.</p> <ul> <li> <p>Leave the swarm <pre><code># Run this on the node\ndocker swarm leave\n\n# Run this on this on another node\ndocker node rm --force [name/ID]\n</code></pre></p> </li> <li> <p>Use terraform to replace the node <pre><code># Got to the directory of the environment with the node you want to replace\ncd terraform/env/prod\n\n# This example is replacing node \"c\"\nterraform apply -replace='module.catalog.module.nodes[\"c\"].aws_instance.node_instance'\n</code></pre></p> </li> <li> <p>Use the AWS console (or be patient) to confirm the new node is ready You can confirm the new node is ready in the EC2 console by viewing the system logs for the instance. This typically takes 5-10 minutes. Or of course, you could just wait that amount of time and skip to the next step hoping for the best!</p> </li> <li> <p>Re-run the last successful provision pipeline This step is required as it will run the playbooks required to configure the newly created node.</p> </li> </ul>"},{"location":"increasing-node-root-partition-size/","title":"Increasing node root partition size","text":"<p>Increasing the size of the root EBS block, used for the root file-system, can be done relatively quickly and without requiring a reboot.</p> <ol> <li>Increase the <code>aws_root_block_size</code> (GB) for each node in <code>terraform/env/prod/main.tf</code></li> <li>Commit and push to <code>main</code> branch. This will trigger <code>terraform</code> to perform the disk expansion via the CI pipeline.<ul> <li>You could also manually run <code>terraform apply</code> on <code>terraform/env/prod/</code>; just be sure to commit and push afterwards.</li> </ul> </li> <li>On each node:<ul> <li>Expand the partition table. This can be done via:   <pre><code>parted --list\n# When prompted to Fix/Ignore, choose fix\nWarning: Not all of the space available to /dev/nvme0n1 appears to be used, you\ncan fix the GPT to use all of the space (an extra 402653184 blocks) or continue\nwith the current setting?\nFix/Ignore? fix\n</code></pre></li> <li>Expand the root partition (typically partition 1; see output of previous command):   <pre><code>parted /dev/nvme0n1\n# Will take you to parted prompt\n(parted) resizepart\nPartition number? 1\nWarning: Partition /dev/nvme0n1p1 is being used. Are you sure you want to continue?\nYes/No? yes\nEnd?  [68.7GB]? 100%\n(parted) quit\n</code></pre></li> <li>Expand the ext4 file-system to use 100% of the partition (default action for <code>resize2fs</code>):   <pre><code>resize2fs /dev/nvme0n1p1\n</code></pre></li> </ul> </li> </ol> <p>At this point the file-system should be expanded. Verify via <code>df -h</code>.</p>"},{"location":"mannually-run-terraform-commands/","title":"Manually run terraform commands","text":"<pre><code># Change directory to the environment you want to apply changes to\ncd terraform/env/prod\n\n# Preview Changes\nterraform plan\n\n# Apply Changes\nterraform apply\n\n# Take down the server cluster and all resources associated with it\nterraform destroy\n\n# Re-initialize new cluster if a node is re-created without removing from swarm first\ndocker swarm init --force-new-cluster \n</code></pre>"},{"location":"mounting-the-shared-storage/","title":"Mounting the Shared Storage","text":"<p>To mount the shared storage (<code>/mnt/shared/local</code> on the nodes) on your local machine, use the credentials for the same user as used by Solr and the Traefik Dashboard). To mount, we will be making use of the <code>sshfs</code> tool.</p> <p>For more information on this share see the technical documentation.</p> <p>Permissions on Server The shared storage on the server will need to be writable by the connecting user. For example, if the storage us group writable by the <code>ubuntu</code> user, the user on the server will needt o be in the <code>ubuntu</code> group. This can be done by adding the group to the user in <code>configure-playbook/variables.yml</code> CI variable, <code>VARIABLE_YAML_FILE</code> and then triggering the CI process. <pre><code> - name: myuser\n   comment: 'Myu Ser'\n   groups: ubuntu\n   public_keys:\n</code></pre></p>"},{"location":"mounting-the-shared-storage/#macos","title":"MacOS","text":"<p>Mac users will require macFUSE and sshfs to be installed separately before continuing. The latest release is available on the osxfuse GitHub site.</p> <p>Locally on the Mac mounting the share, you'll have to make it think it also has permissions to write. For example, if the server has the <code>ubunbu</code> group as writable with gid of <code>1000</code>, you'll need to configure you Mac to also have a group with gid <code>1000</code> that your user</p> <p>To create the new group and add your user <code>myuser</code>: <pre><code># Create group ubuntu with gid 1000\nsudo dscl . -create /groups/ubuntu gid 1000\n# Add user myuser to ubuntu group\nsudo dscl . -append /Groups/ubuntu GroupMembership myuser\n</code></pre></p> <p>From there you should be able to create a directory to mount from and mount as your normal user (no <code>sudo</code>): <pre><code># Only need to create the directory the first time\nmkdir ~/my-sshfs\n\nsshfs -o allow_other,default_permissions myuser@catalog-1.aws.lib.msu.edu:/mnt/shared/local ~/my-sshfs\n</code></pre></p> <p>To unmount: <pre><code>umount ~/my-sshfs \n</code></pre></p>"},{"location":"mounting-the-shared-storage/#linux","title":"Linux","text":"<p>Before attempting to mount, you will need <code>sshfs</code>. To install: <pre><code>$ sudo apt install sshfs\n</code></pre></p> <p>Here is an example of mounting the share (to and example <code>/mnt/point/</code> directory) for a single time (not auto-remounted): <pre><code>sudo sshfs -o allow_other,default_permissions [netid]@catalog-1.aws.lib.msu.edu:/mnt/shared/local /mnt/point\n</code></pre></p> <p>Here is an example of an <code>/etc/fstab</code> entry for mounting it: <pre><code>[netid]@catalog-1.aws.lib.msu.edu:/mnt/shared/local /mnt/point fuse.sshfs noauto,x-systemd.automount,_netdev,reconnect,identityfile=/home/[netid]/.ssh/id_ed25519,allow_other,default_permissions 0 0\n</code></pre></p> <p>To mount: <pre><code>$ sudo mkdir -p /mnt/catalog\n$ sudo mount /mnt/catalog\n$ sudo umount /mnt/catalog\n</code></pre></p>"},{"location":"traefik-lets-encrypt-certificates-not-renewing-automatically/","title":"Traefik Let's Encrypt certificates not renewing automatically","text":"<p>Traefik runs as host network mode on each node and DNS is normally configured to round robin between all nodes on a cluster. This can lead to the situation where the Let's Encrypt certificate fails to renew before expiration due to the round robin DNS not resolving to the correct node when performing a HTTP challenge.</p> <p>This issue is something we have on our radar to investigate further and find a workaround for, but in the mean time it may require a manual intervention to get the certificates to renew before expiration.</p> <p>For context, Let's Encrypt certificates last for 3 months. They normally attempt to renew after 2 months have expired. If a certificate isn't over 2 months old, Traefik will not attempt to renew it.</p> <p>To trigger the Let's Encrypt renewal process within Traefik to succeed for a given node (we'll use <code>catalog-beta.lib.msu.edu</code> for the certificate hostname and <code>catalog-2.aws.lib.msu.edu</code> for the node in this example):</p> <ul> <li>Change DNS for the hostname to the node where the certificate needs to be updated in your local DNS. In our case this is Libraries Windows DNS (manual process, requires Systems unit sysadmin to make change)</li> <li>Example: Update <code>catalog-beta.lib.msu.edu</code> from the round robin production DNS of <code>catalog.aws.lib.msu.edu</code> to the specific node DNS of <code>catalog-2.aws.lib.msu.edu</code></li> <li>If needing to update certs on multiple nodes, or to speed the DNS change back once completed, also reduce the DNS TTL for the record to 1 minute</li> <li>Wait several minutes longer than the original TTL (up to 10 more in some cases) to let DNS changes propagate</li> <li>If the original TTL was 5 minutes, you may need to wait 10 to 15 minutes</li> <li>Connect to the node in question and <code>docker stop</code> the <code>traefik_traefik.xyz...</code> container to force it to restart; restarting Traefik will make it perform a new challenge attempt right away</li> <li>Example: <code>ssh catalog-2.aws.lib.msu.edu</code> and <code>docker stop traefik_traefik.yqeqk81ltw.kaawo44qqtu</code></li> <li>Wait up to two minutes for Traefik to start and the Let's Encrypt challange to complete</li> <li>Verify the new certificate is visible for the host</li> <li>Beware using a browser to verify, as they like to cache everything; consider using the command line.</li> <li>Example: <code>echo | openssl s_client -servername catalog-beta.lib.msu.edu -connect catalog-2.aws.lib.msu.edu:443 2&gt;/dev/null | openssl x509 -nokeys -dates | head -n2</code><ul> <li>Note in the above command where it the certificate hostname and the node hostname are set</li> </ul> </li> <li>If cert if not updated, be patient (10+ minutes) and try again restarting the Traefik container</li> <li>Once the new certificate is verified, proceed to change DNS to update any other certs than need manual assistance</li> <li>Once all certificates are okay, change DNS back to the round robin configuration and prior TTL</li> <li>Example: Update <code>catalog-beta.lib.msu.edu</code> back to <code>catalog.aws.lib.msu.edu</code> with a TTL of 5 minutes</li> </ul> <p>That should be all that's required.</p>"}]}